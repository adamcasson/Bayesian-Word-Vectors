{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, random\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('copperfield.txt') as f:\n",
    "#     raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('data/actors_with_text.csv')[['name', 'iso', 'entity_type', 'sources', 'text', 'keywords']]\n",
    "x.head()\n",
    "x = x.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = ' '.join(list(x['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = re.sub(r'\\s+', ' ', raw_text) # remove extra spacing\n",
    "raw_text = raw_text.lower() # lowercase \n",
    "raw_text = re.sub(r'[^a-zA-Z\\d\\s-]', '', raw_text)\n",
    "raw_text = raw_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate vocabulary and unigram probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = raw_text.split(' ')\n",
    "# raw_text = raw_text[:100000]\n",
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_id = {}\n",
    "id_vocab = {}\n",
    "vocab_probabilities = {}\n",
    "index = 0\n",
    "for word in raw_text:\n",
    "    if word not in vocab_id:\n",
    "        vocab_id[word] = index\n",
    "        id_vocab[index] = word\n",
    "        index += 1\n",
    "    vocab_probabilities[word] = vocab_probabilities.get(word, 0) + (1 / len(raw_text))\n",
    "        \n",
    "len(vocab_id), len(vocab_id) == len(id_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_probabilities['the'], vocab_probabilities['it']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a version of the text with word indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [vocab_id[word] for word in raw_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the training set\n",
    "keys = word index.      \n",
    "values = dictionary where keys are the context word (j) and values are the time that word appears in (i)'s context.    \n",
    "Counts are weighted by -1 for negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = {}\n",
    "window_size = 4\n",
    "neg_pos_ratio = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive examples\n",
    "for text_i, word_i in enumerate(text):\n",
    "    if word_i not in training_data:\n",
    "        training_data[word_i] = {}\n",
    "        \n",
    "    start_window = max(0, text_i - window_size)\n",
    "    end_window = min(len(text), text_i + window_size + 1)\n",
    "        \n",
    "    for text_j in range(start_window, end_window):\n",
    "        word_j = text[text_j]\n",
    "        if text_i != text_j:\n",
    "            training_data[word_i][word_j] = training_data[word_i].get(word_j, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vocab[3], len(training_data[3]), len(training_data) == len(vocab_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative_examples\n",
    "for word_i in tqdm(training_data.keys()):\n",
    "    \n",
    "    found = 0\n",
    "    positive_samples = sum(training_data[word_i].values())\n",
    "    \n",
    "    while found < neg_pos_ratio * positive_samples:\n",
    "        neg_i = random.choice(text)\n",
    "        if (neg_i not in training_data[word_i]) or (training_data[word_i][neg_i] < 0):\n",
    "            training_data[word_i][neg_i] = training_data[word_i].get(neg_i, 0) - 1\n",
    "            found += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum(training_data[3].values()), sum(training_data[3].values()) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    training_data = {}\n",
    "\n",
    "    # positive examples\n",
    "    for text_i, word_i in enumerate(text):\n",
    "        if word_i not in training_data:\n",
    "            training_data[word_i] = {}\n",
    "\n",
    "        start_window = max(0, text_i - window_size)\n",
    "        end_window = min(len(text), text_i + window_size + 1)\n",
    "\n",
    "        for text_j in range(start_window, end_window):\n",
    "            word_j = text[text_j]\n",
    "            if text_i != text_j:\n",
    "                training_data[word_i][word_j] = training_data[word_i].get(word_j, 0) + 1\n",
    "                \n",
    "    # negative_examples\n",
    "    for word_i in training_data.keys():\n",
    "\n",
    "        found = 0\n",
    "        positive_samples = sum(training_data[word_i].values())\n",
    "\n",
    "        while found < neg_pos_ratio * positive_samples:\n",
    "            neg_i = random.choice(text)\n",
    "            if (neg_i not in training_data[word_i]) or (training_data[word_i][neg_i] < 0):\n",
    "                training_data[word_i][neg_i] = training_data[word_i].get(neg_i, 0) - 1\n",
    "                found += 1\n",
    "                \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(i, j):\n",
    "    # calculate cosine similarities.\n",
    "    m_y = np.matmul(i.mean_u.T, j.mean_u) \n",
    "    m_y = m_y / (np.linalg.norm(i.mean_u) * np.linalg.norm(j.mean_u))\n",
    "    var_y = np.matrix.trace(np.matmul(i.covariance_u, j.covariance_u)) \n",
    "    var_y += np.matmul(np.matmul(i.mean_u.T, i.covariance_u), i.mean_u)\n",
    "    var_y += np.matmul(np.matmul(j.mean_u.T, j.covariance_u), j.mean_u)\n",
    "#     var_y = var_y / (np.linalg.norm(i.mean_u) * np.linalg.norm(j.mean_u))\n",
    "    return float(m_y), float(var_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine_similarity(words[1], words[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(i, prnt=None):\n",
    "    wi = words[i]\n",
    "    if prnt: print(wi.text)\n",
    "    info = []\n",
    "    for wj in words:\n",
    "        if wi != wj:\n",
    "            info.append((wj.text, cosine_similarity(wi,wj)))\n",
    "    info.sort(key=lambda x: x[1][0], reverse=True)\n",
    "    \n",
    "    if prnt:\n",
    "        for i in info[:prnt]:\n",
    "            print(i)\n",
    "    else:\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_similar(2)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "#### Initialization stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordData:\n",
    "    def __init__(self, text, m=50):\n",
    "        self.text = text\n",
    "        self.vector_size = m\n",
    "        \n",
    "        self.mean_u = np.random.randn(m,1)\n",
    "        self.mean_v = np.random.randn(m,1)\n",
    "        self.covariance_u = np.identity(m)\n",
    "        self.covariance_v = np.identity(m)\n",
    "        \n",
    "        self.P_u = np.identity(m)\n",
    "        self.P_v = np.identity(m)\n",
    "        self.P_u_new = np.zeros((m,m))\n",
    "        self.P_v_new = np.zeros((m,m))\n",
    "        \n",
    "        self.R_u = np.zeros((m,1))\n",
    "        self.R_v = np.zeros((m,1))\n",
    "        self.R_u_new = np.zeros((m,1))\n",
    "        self.R_v_new = np.zeros((m,1))\n",
    "        \n",
    "    def u_parameter_update(self, beta):\n",
    "        \n",
    "        expr = lambda x, y: beta * x + (1-beta) * y\n",
    "        \n",
    "        # update. \n",
    "        self.R_u = expr(self.R_u_new, self.R_u)\n",
    "        self.P_u = expr(self.P_u_new, self.P_u)\n",
    "        \n",
    "        # u\n",
    "        self.covariance_u = np.linalg.inv(self.P_u)\n",
    "        self.mean_u = np.matmul(self.covariance_u, self.R_u)\n",
    "        self.covariance_u = np.diag(np.diagonal(self.covariance_u))\n",
    "                \n",
    "        # clear new values. \n",
    "        self.R_u_new = np.zeros((m,1))\n",
    "        self.P_u_new = np.zeros((m,m))\n",
    "        \n",
    "    def v_parameter_update(self, beta):\n",
    "        \n",
    "        expr = lambda x, y: beta * x + (1-beta) * y\n",
    "        \n",
    "        # update. \n",
    "        self.R_v = expr(self.R_v_new, self.R_v)\n",
    "        self.P_v = expr(self.P_v_new, self.P_v)\n",
    "        \n",
    "        # v\n",
    "        self.covariance_v = np.linalg.inv(self.P_v)\n",
    "        self.mean_v = np.matmul(self.covariance_v, self.R_v)\n",
    "        self.covariance_v = np.diag(np.diagonal(self.covariance_v))\n",
    "        \n",
    "        # clear new values. \n",
    "        self.R_v_new = np.zeros((m,1))\n",
    "        self.P_v_new = np.zeros((m,m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 40\n",
    "tau = 0.0\n",
    "tau = tau * np.identity(m)\n",
    "gamma = 0.7\n",
    "n_without_update = 5\n",
    "\n",
    "words = [WordData(v, m=m) for v in vocab_id.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-1 * x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(500):\n",
    "    e = 0\n",
    "    training_data = get_data()\n",
    "    beta = 1\n",
    "    if epoch > n_without_update: beta = (epoch-n_without_update) ** (-1 * gamma)\n",
    "    for i,j_dict in tqdm(training_data.items()):\n",
    "\n",
    "        wi = words[i]\n",
    "        \n",
    "        var_wiu = np.expand_dims(np.diagonal(wi.covariance_u), axis=1)\n",
    "        var_wiv = np.expand_dims(np.diagonal(wi.covariance_v), axis=1)\n",
    "        xi_ui = ((var_wiu) + np.square(wi.mean_u))\n",
    "        xi_vi = ((var_wiv) + np.square(wi.mean_v))\n",
    "\n",
    "        for j, d in j_dict.items():\n",
    "            wj = words[j]\n",
    "\n",
    "            # for u\n",
    "            var_wjv = np.expand_dims(np.diagonal(wj.covariance_v), axis=1)\n",
    "            xi = np.matmul(xi_ui.T, (var_wjv + np.square(wj.mean_v)))\n",
    "            xi = np.sqrt(xi)\n",
    "            lambda_xi = (0.5 / xi) * (sigmoid(xi) - 0.5)\n",
    "\n",
    "            eq = wj.covariance_v + np.matmul(wj.mean_v, wj.mean_v.T)\n",
    "            wi.P_u_new += abs(d) * (2 * lambda_xi * eq + tau)\n",
    "            wi.R_u_new += 0.5 * d * wj.mean_v\n",
    "\n",
    "            # for v\n",
    "            var_wju = np.expand_dims(np.diagonal(wj.covariance_u), axis=1)\n",
    "            xi = np.matmul(xi_vi.T, (var_wju + np.square(wj.mean_u)))\n",
    "            xi = np.sqrt(xi)\n",
    "            lambda_xi = (0.5 / xi) * (sigmoid(xi) - 0.5)\n",
    "            \n",
    "            eq = wj.covariance_u + np.matmul(wj.mean_u, wj.mean_u.T)\n",
    "            wi.P_v_new += abs(d) * (2 * lambda_xi * eq + tau)\n",
    "            wi.R_v_new += 0.5 * d * wj.mean_u\n",
    "\n",
    "        e += np.linalg.norm(wi.R_u_new - wi.R_u)\n",
    "        wi.u_parameter_update(beta)\n",
    "        wi.v_parameter_update(beta)\n",
    "        \n",
    "    print(e / len(words))\n",
    "    most_similar(2, prnt=5)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words[1000].mean_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_id['reduce'], vocab_id['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(2760, prnt=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
